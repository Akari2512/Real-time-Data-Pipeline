{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d64a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, glob, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = \"D:/hadoop\"\n",
    "os.environ[\"PATH\"] += os.pathsep + \"D:/hadoop/bin\"\n",
    "os.makedirs(\"D:/hadoop/checkpoint\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d39dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, from_unixtime\n",
    "from pyspark.sql.types import StructType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5801dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫•u h√¨nh th∆∞ m·ª•c\n",
    "output_dir = \"D:/sensor-data/output\"\n",
    "checkpoint_dir = \"D:/hadoop/checkpoint\"\n",
    "backup_dir = \"D:/sensor-data/backup\"\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(backup_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def merge_csv_files_clean_headers(input_dir, output_file):\n",
    "    csv_files = glob.glob(os.path.join(input_dir, \"part-*\"))\n",
    "    if not csv_files:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng c√≥ file ƒë·ªÉ g·ªôp.\")\n",
    "        return None\n",
    "\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file, header=None)\n",
    "            df = df[df[0] != 'sensor_id']\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói ƒë·ªçc file {file}: {e}\")\n",
    "\n",
    "    if dfs:\n",
    "        merged_df = pd.concat(dfs, ignore_index=True)\n",
    "        merged_df.columns = ['sensor_id', 'temperature', 'humidity', 'timestamp']\n",
    "        merged_df.to_csv(output_file, index=False)\n",
    "        print(f\"‚úÖ ƒê√£ g·ªôp v√† l√†m s·∫°ch th√†nh {output_file}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Kh√¥ng c√≥ n·ªôi dung h·ª£p l·ªá ƒë·ªÉ g·ªôp.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Xo√° output v√† checkpoint c≈©\n",
    "def cleanup_dirs():\n",
    "    shutil.rmtree(output_dir, ignore_errors=True)\n",
    "    shutil.rmtree(checkpoint_dir, ignore_errors=True)\n",
    "    print(\"üßπ ƒê√£ xo√° output v√† checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bb20b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def validate_data(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # 1. Ki·ªÉm tra kh√¥ng c√≥ gi√° tr·ªã null\n",
    "        if df['sensor_id'].isnull().any():\n",
    "            print(\"‚ùå C√≥ gi√° tr·ªã thi·∫øu trong 'sensor_id'\")\n",
    "            return False\n",
    "        if df['temperature'].isnull().any():\n",
    "            print(\"‚ùå C√≥ gi√° tr·ªã thi·∫øu trong 'temperature'\")\n",
    "            return False\n",
    "        if df['humidity'].isnull().any():\n",
    "            print(\"‚ùå C√≥ gi√° tr·ªã thi·∫øu trong 'humidity'\")\n",
    "            return False\n",
    "        if df['timestamp'].isnull().any():\n",
    "            print(\"‚ùå C√≥ gi√° tr·ªã thi·∫øu trong 'timestamp'\")\n",
    "            return False\n",
    "\n",
    "        # 2. Ki·ªÉm tra range nhi·ªát ƒë·ªô (-50¬∞C ƒë·∫øn 100¬∞C)\n",
    "        if not df['temperature'].between(-50, 100).all():\n",
    "            print(\"‚ùå Nhi·ªát ƒë·ªô ngo√†i kho·∫£ng h·ª£p l·ªá (-50 ƒë·∫øn 100¬∞C)\")\n",
    "            return False\n",
    "\n",
    "        # 3. Ki·ªÉm tra ƒë·ªô ·∫©m (0% ƒë·∫øn 100%)\n",
    "        if not df['humidity'].between(0, 100).all():\n",
    "            print(\"‚ùå ƒê·ªô ·∫©m ngo√†i kho·∫£ng h·ª£p l·ªá (0 ƒë·∫øn 100%)\")\n",
    "            return False\n",
    "\n",
    "        # 4. Ki·ªÉm tra ƒë·ªãnh d·∫°ng th·ªùi gian (yyyy-mm-dd HH:MM:SS)\n",
    "        try:\n",
    "            pd.to_datetime(df['timestamp'], format=\"%Y-%m-%d %H:%M:%S\", errors='raise')\n",
    "        except ValueError:\n",
    "            print(\"‚ùå ƒê·ªãnh d·∫°ng 'timestamp' kh√¥ng h·ª£p l·ªá\")\n",
    "            return False\n",
    "\n",
    "        print(\"‚úÖ T·∫•t c·∫£ ki·ªÉm tra d·ªØ li·ªáu ƒë·ªÅu ƒë·∫°t y√™u c·∫ßu.\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå L·ªói khi ƒë·ªçc ho·∫∑c ki·ªÉm tra d·ªØ li·ªáu: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "783da17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ ƒê√£ d·ª´ng stream sau 2 ph√∫t.\n"
     ]
    }
   ],
   "source": [
    "# SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSensorConsumerWithValidation\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2\") \\\n",
    "    .config(\"spark.hadoop.home.dir\", \"D:/hadoop\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Schema d·ªØ li·ªáu Kafka\n",
    "schema = StructType() \\\n",
    "    .add('sensor_id', IntegerType()) \\\n",
    "    .add('temperature', DoubleType()) \\\n",
    "    .add('humidity', DoubleType()) \\\n",
    "    .add('timestamp', DoubleType())  # timestamp d·∫°ng Unix\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka\n",
    "df_raw = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
    "    .option('subscribe', 'sensor-data') \\\n",
    "    .option('startingOffsets', 'latest') \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON t·ª´ Kafka v√† chuy·ªÉn timestamp sang d·∫°ng string\n",
    "df_parsed = df_raw.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"timestamp\")).cast(\"string\"))\n",
    "\n",
    "# Ghi d·ªØ li·ªáu ra CSV n·∫øu h·ª£p l·ªá\n",
    "query = df_parsed.writeStream \\\n",
    "    .option(\"path\", output_dir) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .option(\"header\", True) \\\n",
    "    .format(\"csv\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(120)  # 2 ph√∫t\n",
    "query.stop()\n",
    "print(\"üî¥ ƒê√£ d·ª´ng stream sau 2 ph√∫t.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ffe5dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ g·ªôp v√† l√†m s·∫°ch th√†nh D:/sensor-data/backup/merged_20250616_152345.csv\n",
      "‚úÖ T·∫•t c·∫£ ki·ªÉm tra d·ªØ li·ªáu ƒë·ªÅu ƒë·∫°t y√™u c·∫ßu.\n",
      "‚úÖ D·ªØ li·ªáu ƒë·∫°t y√™u c·∫ßu ch·∫•t l∆∞·ª£ng, c√≥ th·ªÉ ƒë∆∞a v√†o SQL/Azure.\n",
      "üßπ ƒê√£ xo√° output v√† checkpoint.\n"
     ]
    }
   ],
   "source": [
    "# ==== G·ªòP FILE + KI·ªÇM TRA D·ªÆ LI·ªÜU ====\n",
    "timestamp_str = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "merged_file_path = f\"{backup_dir}/merged_{timestamp_str}.csv\"\n",
    "\n",
    "# H√†m merge tr·∫£ v·ªÅ True n·∫øu g·ªôp th√†nh c√¥ng\n",
    "success = merge_csv_files_clean_headers(output_dir, merged_file_path)\n",
    "\n",
    "if success and os.path.exists(merged_file_path):\n",
    "    is_clean = validate_data(merged_file_path)\n",
    "    if is_clean:\n",
    "        print(\"‚úÖ D·ªØ li·ªáu ƒë·∫°t y√™u c·∫ßu ch·∫•t l∆∞·ª£ng, c√≥ th·ªÉ ƒë∆∞a v√†o SQL/Azure.\")\n",
    "    else:\n",
    "        print(\"‚ùå D·ªØ li·ªáu KH√îNG ƒë·∫°t ch·∫•t l∆∞·ª£ng. H·ªßy ƒë·∫©y v√†o h·ªá th·ªëng ch√≠nh.\")\n",
    "    cleanup_dirs()  # Xo√° output v√† checkpoint sau khi ki·ªÉm tra\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è G·ªôp file th·∫•t b·∫°i ho·∫∑c kh√¥ng t·ªìn t·∫°i file ƒë·∫ßu ra. B·ªè qua ki·ªÉm tra.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1de7f4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c ƒë·∫©y v√†o SQL Server th√†nh c√¥ng.\n"
     ]
    }
   ],
   "source": [
    "# File CSV sau khi g·ªôp\n",
    "csv_path = merged_file_path\n",
    "\n",
    "# ƒê·ªçc d·ªØ li·ªáu\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# T√™n instance SQL Server trong m√°y b·∫°n (Express ho·∫∑c m·∫∑c ƒë·ªãnh)\n",
    "server = \"LAPTOP-CUA-QUAN\\SQLSERVER1\"  # ho·∫∑c ch·ªâ \"localhost\" n·∫øu d√πng m·∫∑c ƒë·ªãnh\n",
    "database = \"SensorData\"\n",
    "\n",
    "# T·∫°o connection string\n",
    "connection_string = (\n",
    "    f\"mssql+pyodbc://{server}/{database}\"\n",
    "    \"?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "    \"&trusted_connection=yes\"\n",
    ")\n",
    "\n",
    "# T·∫°o engine v√† ƒë·∫©y d·ªØ li·ªáu\n",
    "engine = create_engine(connection_string)\n",
    "df.to_sql('sensor_data', con=engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c ƒë·∫©y v√†o SQL Server th√†nh c√¥ng.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2a88511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ T·∫•t c·∫£ ki·ªÉm tra d·ªØ li·ªáu ƒë·ªÅu ƒë·∫°t y√™u c·∫ßu.\n",
      "üìù ƒê√£ ghi log ETL th√†nh c√¥ng v√†o ETL_Log.\n"
     ]
    }
   ],
   "source": [
    "# Sau khi validate d·ªØ li·ªáu\n",
    "if success and os.path.exists(merged_file_path):\n",
    "    is_clean = validate_data(merged_file_path)\n",
    "    \n",
    "    run_time = pd.Timestamp.now()\n",
    "    record_count = len(df)\n",
    "    job_name = \"KafkaSensorETL\"\n",
    "\n",
    "    if is_clean:\n",
    "        \n",
    "        # Ghi log th√†nh c√¥ng\n",
    "        log_df = pd.DataFrame({\n",
    "            'start_time': [run_time],\n",
    "            'end_time': [run_time],\n",
    "            'job_name': [job_name],\n",
    "            'status': ['Success'],\n",
    "            'records_processed': [record_count],\n",
    "            'error_message': ['Data validation passed']\n",
    "        })\n",
    "        log_df.to_sql('ETL_Log', con=engine, if_exists='append', index=False)\n",
    "        print(\"üìù ƒê√£ ghi log ETL th√†nh c√¥ng v√†o ETL_Log.\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Ghi log th·∫•t b·∫°i v·ªõi th√¥ng b√°o l·ªói\n",
    "        log_df = pd.DataFrame({\n",
    "            'start_time': [run_time],\n",
    "            'end_time': [run_time],\n",
    "            'job_name': [job_name],\n",
    "            'status': ['Failed'],\n",
    "            'records_processed': [record_count],\n",
    "            'error_message': ['Data validation failed']\n",
    "        })\n",
    "        log_df.to_sql('ETL_Log', con=engine, if_exists='append', index=False)\n",
    "        print(\"üìù ƒê√£ ghi log ETL th·∫•t b·∫°i v√†o ETL_Log.\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è G·ªôp file th·∫•t b·∫°i ho·∫∑c kh√¥ng t·ªìn t·∫°i file ƒë·∫ßu ra. B·ªè qua ki·ªÉm tra.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
